{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbe183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='b25a6156-9fd1-4439-ae82-ec812da1b63f', embedding=None, metadata={'page_label': '1', 'file_name': 'eBook-How-to-Build-a-Career-in-AI.pdf', 'file_path': 'eBook-How-to-Build-a-Career-in-AI.pdf', 'file_type': 'application/pdf', 'file_size': 3717673, 'creation_date': '2025-12-02', 'last_modified_date': '2025-12-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='PAGE 1\\nFounder, DeepLearning.AI\\nCollected Insights\\nfrom Andrew Ng\\nHow to \\nBuild\\nYour\\nCareer\\nin AI\\nA Simple Guide\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48688b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main idea of the book is to provide a comprehensive guide for individuals looking to build a successful career in artificial intelligence (AI). It emphasizes the importance of learning foundational technical skills, working on meaningful projects to enhance those skills, and effectively navigating the job search process. The book also discusses the evolving significance of coding, particularly in AI, and offers strategies for overcoming challenges such as imposter syndrome while building a portfolio that demonstrates skill progression.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "index = VectorStoreIndex.from_documents([document], llm=Settings.llm, embed_model=Settings.embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the main idea of the book?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef3abb",
   "metadata": {},
   "source": [
    "##### Evaluation setup using TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5cd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open(\"./eval_questions.txt\", \"r\") as file:\n",
    "    eval_questions = file.readlines()\n",
    "\n",
    "eval_questions = [question.strip() for question in eval_questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246a47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_q = \"How much math should I know to build a career in AI?\"\n",
    "eval_questions.append(new_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769b281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index.core.embeddings.multi_modal_base.MultiModalEmbedding'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index.core.base.embeddings.base.BaseEmbedding'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index.core.schema.TransformComponent'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index.core.schema.BaseComponent'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.embeddings.huggingface.base.HuggingFaceEmbedding'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.callbacks.base.CallbackManager'> for base <class 'llama_index.core.callbacks.base.CallbackManager'>\n",
      "instrumenting <class 'llama_index.core.callbacks.base.CallbackManager'> for base <class 'llama_index.core.callbacks.base_handler.BaseCallbackHandler'>\n",
      "instrumenting <class 'llama_index.core.callbacks.base.CallbackManager'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.callbacks.base.CallbackManager'> for base <class 'object'>\n",
      "instrumenting <class 'bool'> for base <class 'bool'>\n",
      "instrumenting <class 'bool'> for base <class 'int'>\n",
      "instrumenting <class 'bool'> for base <class 'object'>\n",
      "instrumenting <class 'tuple'> for base <class 'tuple'>\n",
      "instrumenting <class 'tuple'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'llama_index.core.vector_stores.types.BasePydanticVectorStore'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'llama_index.core.schema.BaseComponent'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStore'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStoreData'> for base <class 'llama_index.core.vector_stores.simple.SimpleVectorStoreData'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStoreData'> for base <class 'dataclasses_json.api.DataClassJsonMixin'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStoreData'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.vector_stores.simple.SimpleVectorStoreData'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'> for base <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'> for base <class 'llama_index.core.indices.base.BaseIndex'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'> for base <class 'typing.Generic'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'> for base <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'>\n",
      "instrumenting <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'> for base <class 'llama_index.core.graph_stores.types.GraphStore'>\n",
      "instrumenting <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'> for base <class 'typing.Protocol'>\n",
      "instrumenting <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'> for base <class 'typing.Generic'>\n",
      "instrumenting <class 'llama_index.core.graph_stores.simple.SimpleGraphStore'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.node_parser.interface.MetadataAwareTextSplitter'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.node_parser.interface.TextSplitter'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.node_parser.interface.NodeParser'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.schema.TransformComponent'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index.core.schema.BaseComponent'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.node_parser.text.sentence.SentenceSplitter'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'> for base <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'> for base <class 'llama_index.core.storage.docstore.keyval_docstore.KVDocumentStore'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'> for base <class 'llama_index.core.storage.docstore.types.BaseDocumentStore'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.data_structs.data_structs.IndexDict'> for base <class 'llama_index.core.data_structs.data_structs.IndexDict'>\n",
      "instrumenting <class 'llama_index.core.data_structs.data_structs.IndexDict'> for base <class 'llama_index.core.data_structs.data_structs.IndexStruct'>\n",
      "instrumenting <class 'llama_index.core.data_structs.data_structs.IndexDict'> for base <class 'dataclasses_json.api.DataClassJsonMixin'>\n",
      "instrumenting <class 'llama_index.core.data_structs.data_structs.IndexDict'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.data_structs.data_structs.IndexDict'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.types.RefDocInfo'> for base <class 'llama_index.core.storage.docstore.types.RefDocInfo'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.types.RefDocInfo'> for base <class 'dataclasses_json.api.DataClassJsonMixin'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.types.RefDocInfo'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.storage.docstore.types.RefDocInfo'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.storage.storage_context.StorageContext'> for base <class 'llama_index.core.storage.storage_context.StorageContext'>\n",
      "instrumenting <class 'llama_index.core.storage.storage_context.StorageContext'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "\tinstrumenting retrieve\n",
      "\tinstrumenting _retrieve\n",
      "\tinstrumenting _aretrieve\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'llama_index.core.base.base_retriever.BaseRetriever'>\n",
      "\tinstrumenting retrieve\n",
      "\tinstrumenting _retrieve\n",
      "\tinstrumenting _aretrieve\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'llama_index.core.prompts.mixin.PromptMixin'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'str'> for base <class 'str'>\n",
      "instrumenting <class 'str'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index.llms.openai.base.OpenAI'>\n",
      "\tinstrumenting chat\n",
      "\tinstrumenting complete\n",
      "\tinstrumenting stream_chat\n",
      "\tinstrumenting stream_complete\n",
      "\tinstrumenting achat\n",
      "\tinstrumenting acomplete\n",
      "\tinstrumenting astream_chat\n",
      "\tinstrumenting astream_complete\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index.core.llms.function_calling.FunctionCallingLLM'>\n",
      "\tinstrumenting chat\n",
      "\tinstrumenting complete\n",
      "\tinstrumenting stream_chat\n",
      "\tinstrumenting stream_complete\n",
      "\tinstrumenting achat\n",
      "\tinstrumenting acomplete\n",
      "\tinstrumenting astream_chat\n",
      "\tinstrumenting astream_complete\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index.core.llms.llm.LLM'>\n",
      "\tinstrumenting chat\n",
      "\tinstrumenting complete\n",
      "\tinstrumenting stream_chat\n",
      "\tinstrumenting stream_complete\n",
      "\tinstrumenting achat\n",
      "\tinstrumenting acomplete\n",
      "\tinstrumenting astream_chat\n",
      "\tinstrumenting astream_complete\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index.core.base.llms.base.BaseLLM'>\n",
      "\tinstrumenting chat\n",
      "\tinstrumenting complete\n",
      "\tinstrumenting stream_chat\n",
      "\tinstrumenting stream_complete\n",
      "\tinstrumenting achat\n",
      "\tinstrumenting acomplete\n",
      "\tinstrumenting astream_chat\n",
      "\tinstrumenting astream_complete\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index.core.schema.BaseComponent'>\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.llms.openai.base.OpenAI'> for base <class 'object'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'llama_index.core.base.llms.types.LLMMetadata'> for base <class 'llama_index.core.base.llms.types.LLMMetadata'>\n",
      "instrumenting <class 'llama_index.core.base.llms.types.LLMMetadata'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.base.llms.types.LLMMetadata'> for base <class 'object'>\n",
      "instrumenting <class 'tuple'> for base <class 'tuple'>\n",
      "instrumenting <class 'tuple'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.indices.prompt_helper.PromptHelper'> for base <class 'llama_index.core.indices.prompt_helper.PromptHelper'>\n",
      "instrumenting <class 'llama_index.core.indices.prompt_helper.PromptHelper'> for base <class 'llama_index.core.schema.BaseComponent'>\n",
      "instrumenting <class 'llama_index.core.indices.prompt_helper.PromptHelper'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.indices.prompt_helper.PromptHelper'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'>\n",
      "\tinstrumenting get_response\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'llama_index.core.response_synthesizers.refine.Refine'>\n",
      "\tinstrumenting get_response\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'llama_index.core.response_synthesizers.base.BaseSynthesizer'>\n",
      "\tinstrumenting get_response\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'llama_index.core.prompts.mixin.PromptMixin'>\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.response_synthesizers.compact_and_refine.CompactAndRefine'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'llama_index.core.prompts.base.SelectorPromptTemplate'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'llama_index.core.prompts.base.BasePromptTemplate'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'llama_index.core.prompts.base.SelectorPromptTemplate'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'llama_index.core.prompts.base.BasePromptTemplate'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'pydantic.main.BaseModel'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.prompts.base.SelectorPromptTemplate'> for base <class 'object'>\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "\tinstrumenting query\n",
      "\tinstrumenting aquery\n",
      "\tinstrumenting synthesize\n",
      "\tinstrumenting asynthesize\n",
      "\tinstrumenting retrieve\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'llama_index.core.base.base_query_engine.BaseQueryEngine'>\n",
      "\tinstrumenting query\n",
      "\tinstrumenting aquery\n",
      "\tinstrumenting synthesize\n",
      "\tinstrumenting asynthesize\n",
      "\tinstrumenting retrieve\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'llama_index.core.prompts.mixin.PromptMixin'>\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'llama_index_instrumentation.DispatcherSpanMixin'>\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'abc.ABC'>\n",
      "instrumenting <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'> for base <class 'object'>\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "These package is required for evaluating feedback using OpenAI:\n\n    trulens-providers-openai\n\nYou should be able to install it with pip:\n\n    ```bash\n    pip install \"trulens-providers-openai>=1.0.0\"\n    ```",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.schema'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m tru.reset_database()\n\u001b[32m     11\u001b[39m tru_recorder = get_prebuilt_trulens_recorder(query_engine, \u001b[33m\"\u001b[39m\u001b[33mDirect Query Evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[32m     15\u001b[39m         response = query_engine.query(question)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/app.py:1186\u001b[39m, in \u001b[36mApp.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, exc_tb)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28mself\u001b[39m.recording_contexts.reset(ctx.token)\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tru_recorder \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m eval_questions:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         response = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m records, feedbacks = tru.get_records_and_feedback(app_ids=[])\n\u001b[32m     18\u001b[39m records.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1180\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001b[32m   1178\u001b[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrewrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1164\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001b[39m\u001b[34m(rets)\u001b[39m\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m python_utils.WRAP_LAZY:\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m         rets = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_lazy_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mon_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_call_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m     update_call_info(rets, final=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/apps/llamaindex/tru_llama.py:643\u001b[39m, in \u001b[36mTruLlama.wrap_lazy_values\u001b[39m\u001b[34m(self, rets, wrap, on_done, context_vars)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rets\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mon_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1007\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001b[39m\u001b[34m(rets, final)\u001b[39m\n\u001b[32m   1001\u001b[39m         cost = tally()  \u001b[38;5;66;03m# get updated cost\u001b[39;00m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stack) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m existing_record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1004\u001b[39m         \u001b[38;5;66;03m# If this is a root call, notify app to add the completed record\u001b[39;00m\n\u001b[32m   1005\u001b[39m         \u001b[38;5;66;03m# into its containers:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m         records[ctx] = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_add_record\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m            \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m            \u001b[49m\u001b[43msig\u001b[49m\u001b[43m=\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m            \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m            \u001b[49m\u001b[43mperf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_schema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPerf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_time\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexisting_record\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexisting_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/app.py:1338\u001b[39m, in \u001b[36mApp.on_add_record\u001b[39m\u001b[34m(self, ctx, func, sig, bindings, ret, error, perf, cost, existing_record, final)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1336\u001b[39m     \u001b[38;5;66;03m# May block on DB.\u001b[39;00m\n\u001b[32m   1337\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_error(record=record, error=error)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1340\u001b[39m \u001b[38;5;66;03m# Only continue with the feedback steps if the record is final.\u001b[39;00m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:921\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001b[32m    919\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     rets, tally = \u001b[43mcore_endpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    926\u001b[39m     error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:574\u001b[39m, in \u001b[36mEndpoint.track_all_costs_tally\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrack_all_costs_tally\u001b[39m(\n\u001b[32m    553\u001b[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m     **kwargs,\n\u001b[32m    562\u001b[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001b[32m    563\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[33;03m    execution of thunk.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m \u001b[33;03m            change after this method returns in case of Awaitable results.\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     result, cbs = \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) == \u001b[32m0\u001b[39m:\n\u001b[32m    587\u001b[39m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[32m    588\u001b[39m         tally = \u001b[38;5;28;01mlambda\u001b[39;00m: base_schema.Cost()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:547\u001b[39m, in \u001b[36mEndpoint.track_all_costs\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    539\u001b[39m             logger.debug(\n\u001b[32m    540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPossibly missing key(s). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m                 e,\n\u001b[32m    545\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:651\u001b[39m, in \u001b[36mEndpoint._track_costs\u001b[39m\u001b[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[39m\n\u001b[32m    646\u001b[39m context_vars = {\n\u001b[32m    647\u001b[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001b[32m    648\u001b[39m }\n\u001b[32m    650\u001b[39m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m result: T = \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewrap\u001b[39m(result):\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m python_utils.is_lazy(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/base/base_query_engine.py:44\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     43\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m dispatcher.event(\n\u001b[32m     46\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/query_engine/retriever_query_engine.py:197\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    194\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    195\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m    196\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_synthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/response_synthesizers/base.py:235\u001b[39m, in \u001b[36mBaseSynthesizer.synthesize\u001b[39m\u001b[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m     query = QueryBundle(query_str=query)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager.event(\n\u001b[32m    232\u001b[39m     CBEventType.SYNTHESIZE,\n\u001b[32m    233\u001b[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001b[32m    234\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     response_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetadataMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     additional_source_nodes = additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    244\u001b[39m     source_nodes = \u001b[38;5;28mlist\u001b[39m(nodes) + \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1180\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001b[32m   1178\u001b[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrewrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1164\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001b[39m\u001b[34m(rets)\u001b[39m\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m python_utils.WRAP_LAZY:\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m         rets = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_lazy_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mon_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_call_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m     update_call_info(rets, final=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/apps/llamaindex/tru_llama.py:643\u001b[39m, in \u001b[36mTruLlama.wrap_lazy_values\u001b[39m\u001b[34m(self, rets, wrap, on_done, context_vars)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rets\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mon_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1023\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001b[39m\u001b[34m(rets, final)\u001b[39m\n\u001b[32m   1007\u001b[39m         records[ctx] = ctx.app.on_add_record(\n\u001b[32m   1008\u001b[39m             ctx=ctx,\n\u001b[32m   1009\u001b[39m             func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m             final=final,\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:921\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001b[32m    919\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     rets, tally = \u001b[43mcore_endpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    926\u001b[39m     error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:574\u001b[39m, in \u001b[36mEndpoint.track_all_costs_tally\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrack_all_costs_tally\u001b[39m(\n\u001b[32m    553\u001b[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m     **kwargs,\n\u001b[32m    562\u001b[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001b[32m    563\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[33;03m    execution of thunk.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m \u001b[33;03m            change after this method returns in case of Awaitable results.\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     result, cbs = \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) == \u001b[32m0\u001b[39m:\n\u001b[32m    587\u001b[39m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[32m    588\u001b[39m         tally = \u001b[38;5;28;01mlambda\u001b[39;00m: base_schema.Cost()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:547\u001b[39m, in \u001b[36mEndpoint.track_all_costs\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    539\u001b[39m             logger.debug(\n\u001b[32m    540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPossibly missing key(s). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m                 e,\n\u001b[32m    545\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:651\u001b[39m, in \u001b[36mEndpoint._track_costs\u001b[39m\u001b[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[39m\n\u001b[32m    646\u001b[39m context_vars = {\n\u001b[32m    647\u001b[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001b[32m    648\u001b[39m }\n\u001b[32m    650\u001b[39m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m result: T = \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewrap\u001b[39m(result):\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m python_utils.is_lazy(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:43\u001b[39m, in \u001b[36mCompactAndRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_texts = \u001b[38;5;28mself\u001b[39m._make_compact_text_chunks(query_str, text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1180\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001b[32m   1178\u001b[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrewrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1164\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001b[39m\u001b[34m(rets)\u001b[39m\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m python_utils.WRAP_LAZY:\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m         rets = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_lazy_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mon_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_call_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m     update_call_info(rets, final=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/apps/llamaindex/tru_llama.py:643\u001b[39m, in \u001b[36mTruLlama.wrap_lazy_values\u001b[39m\u001b[34m(self, rets, wrap, on_done, context_vars)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rets\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mon_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1023\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001b[39m\u001b[34m(rets, final)\u001b[39m\n\u001b[32m   1007\u001b[39m         records[ctx] = ctx.app.on_add_record(\n\u001b[32m   1008\u001b[39m             ctx=ctx,\n\u001b[32m   1009\u001b[39m             func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m             final=final,\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:921\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001b[32m    919\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     rets, tally = \u001b[43mcore_endpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    926\u001b[39m     error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:574\u001b[39m, in \u001b[36mEndpoint.track_all_costs_tally\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrack_all_costs_tally\u001b[39m(\n\u001b[32m    553\u001b[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m     **kwargs,\n\u001b[32m    562\u001b[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001b[32m    563\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[33;03m    execution of thunk.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m \u001b[33;03m            change after this method returns in case of Awaitable results.\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     result, cbs = \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) == \u001b[32m0\u001b[39m:\n\u001b[32m    587\u001b[39m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[32m    588\u001b[39m         tally = \u001b[38;5;28;01mlambda\u001b[39;00m: base_schema.Cost()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:547\u001b[39m, in \u001b[36mEndpoint.track_all_costs\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    539\u001b[39m             logger.debug(\n\u001b[32m    540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPossibly missing key(s). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m                 e,\n\u001b[32m    545\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:651\u001b[39m, in \u001b[36mEndpoint._track_costs\u001b[39m\u001b[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[39m\n\u001b[32m    646\u001b[39m context_vars = {\n\u001b[32m    647\u001b[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001b[32m    648\u001b[39m }\n\u001b[32m    650\u001b[39m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m result: T = \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewrap\u001b[39m(result):\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m python_utils.is_lazy(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/response_synthesizers/refine.py:179\u001b[39m, in \u001b[36mRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_give_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._refine_response_single(\n\u001b[32m    185\u001b[39m             prev_response, query_str, text_chunk, **response_kwargs\n\u001b[32m    186\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/response_synthesizers/refine.py:241\u001b[39m, in \u001b[36mRefine._give_response_single\u001b[39m\u001b[34m(self, query_str, text_chunk, **response_kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    239\u001b[39m         structured_response = cast(\n\u001b[32m    240\u001b[39m             StructuredRefineResponse,\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         query_satisfied = structured_response.query_satisfied\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/response_synthesizers/refine.py:85\u001b[39m, in \u001b[36mDefaultRefineProgram.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m     83\u001b[39m         answer = answer.model_dump_json()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer=answer, query_satisfied=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/llms/llm.py:623\u001b[39m, in \u001b[36mLLM.predict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    622\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1180\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001b[32m   1178\u001b[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrewrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1164\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001b[39m\u001b[34m(rets)\u001b[39m\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m python_utils.WRAP_LAZY:\n\u001b[32m   1163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m         rets = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_lazy_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mon_done\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_call_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1171\u001b[39m     update_call_info(rets, final=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/apps/llamaindex/tru_llama.py:643\u001b[39m, in \u001b[36mTruLlama.wrap_lazy_values\u001b[39m\u001b[34m(self, rets, wrap, on_done, context_vars)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rets\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mon_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:1023\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001b[39m\u001b[34m(rets, final)\u001b[39m\n\u001b[32m   1007\u001b[39m         records[ctx] = ctx.app.on_add_record(\n\u001b[32m   1008\u001b[39m             ctx=ctx,\n\u001b[32m   1009\u001b[39m             func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m             final=final,\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/instruments.py:921\u001b[39m, in \u001b[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001b[32m    919\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     rets, tally = \u001b[43mcore_endpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs_tally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    926\u001b[39m     error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:574\u001b[39m, in \u001b[36mEndpoint.track_all_costs_tally\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrack_all_costs_tally\u001b[39m(\n\u001b[32m    553\u001b[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m     **kwargs,\n\u001b[32m    562\u001b[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001b[32m    563\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[33;03m    execution of thunk.\u001b[39;00m\n\u001b[32m    565\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    571\u001b[39m \u001b[33;03m            change after this method returns in case of Awaitable results.\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     result, cbs = \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_all_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_hugs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_litellm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_bedrock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cortex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_dummy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cbs) == \u001b[32m0\u001b[39m:\n\u001b[32m    587\u001b[39m         \u001b[38;5;66;03m# Otherwise sum returns \"0\" below.\u001b[39;00m\n\u001b[32m    588\u001b[39m         tally = \u001b[38;5;28;01mlambda\u001b[39;00m: base_schema.Cost()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:547\u001b[39m, in \u001b[36mEndpoint.track_all_costs\u001b[39m\u001b[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    539\u001b[39m             logger.debug(\n\u001b[32m    540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mCould not initialize endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPossibly missing key(s). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m                 e,\n\u001b[32m    545\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEndpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_track_costs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_endpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:651\u001b[39m, in \u001b[36mEndpoint._track_costs\u001b[39m\u001b[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001b[39m\n\u001b[32m    646\u001b[39m context_vars = {\n\u001b[32m    647\u001b[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001b[32m    648\u001b[39m }\n\u001b[32m    650\u001b[39m \u001b[38;5;66;03m# Call the function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m result: T = \u001b[43m__func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewrap\u001b[39m(result):\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m python_utils.is_lazy(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/core/llms/callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/llms/openai/base.py:392\u001b[39m, in \u001b[36mOpenAI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    391\u001b[39m     chat_fn = completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m._complete)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/llms/openai/base.py:112\u001b[39m, in \u001b[36mllm_retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    105\u001b[39m retry = create_retry_decorator(\n\u001b[32m    106\u001b[39m     max_retries=max_retries,\n\u001b[32m    107\u001b[39m     random_exponential=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     max_seconds=\u001b[32m20\u001b[39m,\n\u001b[32m    111\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.9/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.9/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/llama_index/llms/openai/base.py:488\u001b[39m, in \u001b[36mOpenAI._chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m message_dicts = to_openai_message_dicts(\n\u001b[32m    483\u001b[39m     messages,\n\u001b[32m    484\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    485\u001b[39m )\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reuse_client:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:850\u001b[39m, in \u001b[36mEndpoint.wrap_function.<locals>.tru_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Problem here: OpenAI returns generators inside its own special\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# classes. These are thus handled in\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# OpenAIEndpoint.wrapped_call .\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/feedback/endpoint.py:832\u001b[39m, in \u001b[36mEndpoint.wrap_function.<locals>.tru_wrapper.<locals>.update_response\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    829\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mHandling endpoint \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, endpoint.name)\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m python_utils.with_context(context_vars):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     response_ = \u001b[43mendpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    839\u001b[39m         \u001b[38;5;66;03m# Handler is allowed to override the response in\u001b[39;00m\n\u001b[32m    840\u001b[39m         \u001b[38;5;66;03m# case it wants to wrap some generators or similar\u001b[39;00m\n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# lazy structures.\u001b[39;00m\n\u001b[32m    842\u001b[39m         response = response_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/providers/openai/endpoint.py:461\u001b[39m, in \u001b[36mOpenAIEndpoint.handle_wrapped_call\u001b[39m\u001b[34m(self, func, bindings, response, callback)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    460\u001b[39m     callbacks.append(callback)\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/providers/openai/endpoint.py:560\u001b[39m, in \u001b[36mOpenAIEndpoint._handle_response\u001b[39m\u001b[34m(model_name, response, callbacks)\u001b[39m\n\u001b[32m    555\u001b[39m         reasoning_tokens = output_details.get(\n\u001b[32m    556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_tokens\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    557\u001b[39m         )\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# See how to construct in langchain.llms.openai.OpenAIChat._generate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m llm_res = \u001b[43mLLMResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[32m    566\u001b[39m     callback.handle_generation(response=llm_res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/AI-LLM/advanced-rag/.venv/lib/python3.13/site-packages/trulens/core/utils/imports.py:437\u001b[39m, in \u001b[36mDummy.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exception_class(\u001b[38;5;28mself\u001b[39m.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mself\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moriginal_exception\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: These package is required for evaluating feedback using OpenAI:\n\n    trulens-providers-openai\n\nYou should be able to install it with pip:\n\n    ```bash\n    pip install \"trulens-providers-openai>=1.0.0\"\n    ```"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "from utils import get_prebuilt_trulens_recorder\n",
    "import os\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"0\"\n",
    "\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(query_engine, \"Direct Query Evaluation\")\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    for question in eval_questions:\n",
    "        response = query_engine.query(question)\n",
    "\n",
    "records, feedbacks = tru.get_records_and_feedback(app_ids=[])\n",
    "records.head()\n",
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
